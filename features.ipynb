{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image features exercise\n",
    "\n",
    "In this exercise we will show that we can improve our classification performance by training  classifiers not on raw pixels but on features that are computed from the raw pixels.\n",
    "\n",
    "All of your work for this exercise will be done in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from camalab.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15., 12.) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Similar to previous exercises, we will load CIFAR-10 data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000L, 32L, 32L, 3L)\n",
      "(500L, 32L, 32L, 3L)\n",
      "(500L, 32L, 32L, 3L)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=5000, num_validation=500, num_test=500):\n",
    "  # Load the raw CIFAR-10 data\n",
    "  cifar10_dir = 'camalab/datasets/cifar-10-batches-py' # you should change it to your own path, \n",
    "                                                      # or put the dataset to this path  \n",
    "    \n",
    "  X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir, 3)\n",
    "  \n",
    "  # Subsample the data\n",
    "  mask = range(num_training, num_training + num_validation)\n",
    "  X_val = X_train[mask]\n",
    "  y_val = y_train[mask]\n",
    "  mask = range(num_training)\n",
    "  X_train = X_train[mask]\n",
    "  y_train = y_train[mask]\n",
    "  mask = range(num_test)\n",
    "  X_test = X_test[mask]\n",
    "  y_test = y_test[mask]\n",
    "\n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print X_train.shape\n",
    "print X_val.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features\n",
    "For each image we will compute a Histogram of Oriented\n",
    "Gradients (HOG) as well as a color histogram. We form our final feature vector for each image by concatenating\n",
    "the HOG and color histogram feature vectors.\n",
    "\n",
    "Roughly speaking, HOG should capture the texture of the image while ignoring\n",
    "color information, and the color histogram represents the color of the input\n",
    "image while ignoring texture. As a result, we expect that using both together\n",
    "ought to work better than using either alone. Verifying this assumption would\n",
    "be a good thing to try for the bonus section.\n",
    "\n",
    "You should import your `hog` and `color_histogram` functions. Both operate on a single\n",
    "image and return a feature vector for that image. \n",
    "\n",
    "Your function should takes a set of images and a list of feature functions and evaluates each feature function on each image, storing the results in a matrix where each column is the concatenation of all feature vectors for a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import uniform_filter\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(imgs, feature_fns, verbose=False):\n",
    "  num_images = imgs.shape[0]\n",
    "  if num_images == 0:\n",
    "    return np.array([])\n",
    "\n",
    "  # 确定第一个图像特征维度\n",
    "  feature_dims = []\n",
    "  first_image_features = []\n",
    "  for feature_fn in feature_fns:\n",
    "    feats = feature_fn(imgs[0].squeeze())\n",
    "    assert len(feats.shape) == 1 #特征函数1维\n",
    "    feature_dims.append(feats.size)\n",
    "    first_image_features.append(feats)\n",
    "\n",
    "  # 分配\n",
    "  total_feature_dim = sum(feature_dims)\n",
    "  imgs_features = np.zeros((num_images, total_feature_dim))\n",
    "  imgs_features[0] = np.hstack(first_image_features).T\n",
    "\n",
    "  # 提取其余图像特征\n",
    "  for i in xrange(1, num_images):  #与range的差别，可以逐一替代\n",
    "    idx = 0\n",
    "    for feature_fn, feature_dim in zip(feature_fns, feature_dims):\n",
    "      next_idx = idx + feature_dim\n",
    "      imgs_features[i, idx:next_idx] = feature_fn(imgs[i].squeeze())\n",
    "      idx = next_idx\n",
    "\n",
    "  return imgs_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hog_feature\n",
    "\n",
    "1）灰度化（将图像看做一个x,y,z（灰度）的三维图像）；\n",
    "\n",
    "2）采用Gamma校正法对输入图像进行颜色空间的标准化（归一化）；目的是调节图像的对比度，降低图像局部的阴影和光照变化所造成的影响，同时可以抑制噪音的干扰；\n",
    "\n",
    "3）计算图像每个像素的梯度（包括大小和方向）；主要是为了捕获轮廓信息，同时进一步弱化光照的干扰。\n",
    "\n",
    "4）将图像划分成小cells（例如6*6像素/cell）；\n",
    "\n",
    "5）统计每个cell的梯度直方图（不同梯度的个数），即可形成每个cell的descriptor；\n",
    "\n",
    "6）将每几个cell组成一个block（例如3*3个cell/block），一个block内所有cell的特征descriptor串联起来便得到该block的HOG特征descriptor。\n",
    "\n",
    "7）将图像image内的所有block的HOG特征descriptor串联起来就可以得到该image（你要检测的目标）的HOG特征descriptor了。这个就是最终的可供分类使用的特征向量了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#假设每个block块都包含2*2个cell，默认步长也为2,设定cell的360度方向分成8个方向块\n",
    "def hog_feature(im):\n",
    "\n",
    "  im = np.dot(im[...,:3], [0.299, 0.587, 0.144])# 灰度图参数\n",
    "  figure_x, figure_y = im.shape\n",
    "  cell_x,cell_y = (8,8)\n",
    "  grad_x = np.zeros((figure_x, figure_y))#初始化数组\n",
    "  grad_y = np.zeros((figure_x, figure_y))\n",
    "  grad = np.zeros((figure_x, figure_y, 2))\n",
    "\n",
    "  for i in xrange(1, figure_x - 1):\n",
    "    for j in xrange(1, figure_y - 1):\n",
    "      grad_x[i, j] = im[i, j - 1] - im[i, j + 1]\n",
    "      grad_y[i, j] = im[i + 1, j] - im[i - 1, j]\n",
    "      grad_dir[i, j] = (np.arctan(grad_y[i, j] / (grad_x[i, j] + 1e-5)) / np.pi) * 180   # 梯度方向\n",
    "      if grad_x[i, j] < 0:\n",
    "         grad_dir[i, j] = grad_dir[i, j]+180\n",
    "      grad_dir[i, j] = (grad_dir[i, j] + 360) % 360\n",
    "      grad_ori[i, j] = np.sqrt(grad_x[i, j] ** 2 + grad_y[i, j] ** 2) # 梯度幅值\n",
    "\n",
    "  cellx_count = int(np.floor(cell_x))  # 向下取整\n",
    "  celly_count = int(np.floor(cell_y))\n",
    "  \n",
    "  blockx_count = (figure_x - 2 * cell_x) / 2 + 1# 块的数量\n",
    "  blocky_count = (figure_y - 2 * cell_y) / 2 + 1\n",
    "\n",
    "  HOG = np.zeros((blocky_count, blockx_count, 2 * 2 * 8))#初始化HOG\n",
    "  for y in xrange(blocky_count):   #全部覆盖一遍\n",
    "    for x in xrange(blockx_count):\n",
    "      block_h = np.zeros(32)\n",
    "      block = grad[y * 2:y * 2 + 16, x * 2:x * 2 + 16]# 块\n",
    "      for k in xrange(2):\n",
    "        for m in xrange(2): \n",
    "          cell_h = np.zeros(8)\n",
    "          cell = block[k * 8:(k + 1) * 8, m * 8:(m + 1) * 8]# cell\n",
    "          for i in xrange(celly_count):\n",
    "            for j in xrange(cellx_count):\n",
    "              cell_h[int(cell[i, j, 0] / 45)] += cell[i, j, 1]\n",
    "          block_h[(k *2 + m) * 8:(k * 2 + m + 1) * 8] = cell_h[:]\n",
    "      HOG[y, x, :] = block_h / np.sqrt(block_h.sum() ** 2 + 1e-5)\n",
    "  HOG = HOG.ravel()\n",
    "  return HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hog_feature_fun(images):\n",
    "    Hog_images=np.zeros((len(images),))\n",
    "    for i in range(len(images)):\n",
    "        Hog_images[i]=hog_feature(images[i])\n",
    "    return Hog_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### color_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'grad_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-2f78c21825d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mnum_color_bins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;31m# Number of bins in the color histogram\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mfeature_fns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mhog_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcolor_histogram_hsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_color_bins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mX_train_feats1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_fns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mX_val_feats1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_fns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mX_test_feats1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_fns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-6ef6eecc4020>\u001b[0m in \u001b[0;36mextract_features\u001b[1;34m(imgs, feature_fns, verbose)\u001b[0m\n\u001b[0;32m      8\u001b[0m   \u001b[0mfirst_image_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mfeature_fn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_fns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mfeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m#特征函数1维\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mfeature_dims\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-52c750fa7029>\u001b[0m in \u001b[0;36mhog_feature\u001b[1;34m(im)\u001b[0m\n\u001b[0;32m     13\u001b[0m       \u001b[0mgrad_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m       \u001b[0mgrad_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m       \u001b[0mgrad_dir\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marctan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgrad_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m180\u001b[0m   \u001b[1;31m# 梯度方向\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mgrad_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m          \u001b[0mgrad_dir\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_dir\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m180\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'grad_dir' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def color_histogram_hsv(im, nbin=10, xmin=0, xmax=255, normalized=True):\n",
    "  ndim = im.ndim\n",
    "  bins = np.linspace(xmin, xmax, nbin+1)\n",
    "  hsv = colors.rgb_to_hsv(im/xmax) * xmax\n",
    "  imhist, bin_edges = np.histogram(hsv[:,:,0], bins=bins, density=normalized)\n",
    "  imhist = imhist * np.diff(bin_edges) \n",
    "  return imhist\n",
    "\n",
    "# color_histogram\n",
    "num_color_bins = 10 # Number of bins in the color histogram\n",
    "feature_fns = [hog_feature, lambda img: color_histogram_hsv(img, nbin=num_color_bins)]\n",
    "X_train_feats1 = extract_features(X_train, feature_fns, verbose=True)\n",
    "X_val_feats1 = extract_features(X_val, feature_fns)\n",
    "X_test_feats1 = extract_features(X_test, feature_fns)\n",
    "\n",
    "# HOG\n",
    "X_train_feats2 = hog_feature_fun(X_train)  \n",
    "X_val_feats2 = hog_feature_fun(X_val)\n",
    "X_test_feats2 = hog_feature_fun(X_test)\n",
    "\n",
    "# Concatenating the HOG and color histogram feature vectors.\n",
    "X_train_feats = np.concatenate((X_train_feats1,X_train_feats2),axis=1)  \n",
    "X_val_feats =  np.concatenate((X_val_feats1,X_val_feats2), axis=1)\n",
    "X_test_feats =  np.concatenate((X_test_feats1,X_test_feats2), axis=1)\n",
    "\n",
    "# Preprocessing: Subtract the mean feature\n",
    "mean_feat = np.mean(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats -= mean_feat  \n",
    "X_val_feats -= mean_feat\n",
    "X_test_feats -= mean_feat\n",
    "\n",
    "# Preprocessing: Divide by standard deviation. This ensures that each feature\n",
    "# has roughly the same scale.\n",
    "std_feat = np.std(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats /= std_feat\n",
    "X_val_feats /= std_feat\n",
    "X_test_feats /= std_feat\n",
    "\n",
    "# Preprocessing: Add a bias dimension\n",
    "# In k-NN, the bias dimension is useless. But you may need it in other classifiers.\n",
    "X_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])\n",
    "X_val_feats = np.hstack([X_val_feats, np.ones((X_val_feats.shape[0], 1))])\n",
    "X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN on features\n",
    "Using the k-NN code developed earlier in the assignment. This should achieve better results than k-NN directly on top of raw pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camalab.classifiers import KNearestNeighbor\n",
    "\n",
    "classifier = KNearestNeighbor()\n",
    "classifier.train(X_train_feats, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the validation set to tune the 'k'\n",
    "\n",
    "k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]\n",
    "k_to_accuracies = {} # your should store the results in this dict\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# You must use the simple validation method. Because we have divided the       #\n",
    "# dataset to validation set and train set.                                     #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "for k in sorted(k_to_accuracies):\n",
    "    print 'k = %d, accuracy = %f' % (k, k_to_accuracies[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your classifier\n",
    "\n",
    "best_k = 1  # choose your best k\n",
    "\n",
    "# Compute and display the accuracy\n",
    "y_test_pred = classifier.predict(X_test_feats, k=best_k)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
